{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e936a3e",
   "metadata": {},
   "source": [
    "Import all modules you'll need. if you get an error like : no module \"module_name\" found; just run \"pip install module_name\" in empty cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26953e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib Qt\n",
    "%load_ext autoreload \n",
    "# to automaticaly reload all the modules that have been modified\n",
    "%autoreload 2 \n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, ShuffleSplit\n",
    "from sklearn import svm, kernel_ridge, metrics, preprocessing\n",
    "from PIL import Image, ExifTags, ImageFile\n",
    "import visualize as vis\n",
    "import preprocessing as pp\n",
    "import processing as proc\n",
    "import calculate as cc\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import correction as corr\n",
    "from read_WDF import read_WDF, get_exif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1905f4",
   "metadata": {},
   "source": [
    "### Reading\n",
    "\n",
    "In os.chdir, put the enplacement of your .wdf file and run the cell to get all your wdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86565303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  10nA_5h_obj10.wdf\n",
      "1 )  800C_355nm.wdf\n",
      "2 )  bida-a_pt1.wdf\n",
      "3 )  bida-a_pt2.wdf\n",
      "4 )  bida-a_pt3.wdf\n",
      "5 )  bida-a_pt4.wdf\n",
      "6 )  bida-a_pt5.wdf\n",
      "7 )  B_carto_rayon_633nm_50%.wdf\n",
      "8 )  Ca675A325_514nm.wdf\n",
      "9 )  carto_10238_20x_HD1.wdf\n",
      "10 )  carto_10238_20x_tr.wdf\n",
      "11 )  carto_10291.wdf\n",
      "12 )  carto_50x-bacteries-stramline.wdf\n",
      "13 )  Carto_plaque_M3C0_7x7cm.wdf\n",
      "14 )  carto_Z0_CR_NF3_curve fitting.wdf\n",
      "15 )  cit-ACP1_chauffe_25-400°C_488nm_1-34sp.wdf\n",
      "16 )  depth_Raoul.wdf\n",
      "17 )  line.wdf\n",
      "18 )  mapping.wdf\n",
      "19 )  nax31_C_zone3_carto100x.wdf\n",
      "20 )  ref-big-carto_633nm-50�-1_CCR.wdf\n",
      "21 )  ref-big-carto_633nm-50�-1_CCR_Copy1.wdf\n",
      "22 )  SBN4_ech3_poli-ethe_carto-ponctuelle.wdf\n",
      "23 )  silice_LR.wdf\n",
      "24 )  Spectre_M3C0_514nm.wdf\n",
      "25 )  Streamline.wdf\n",
      "26 )  test.wdf\n",
      "27 )  UO2 vierge LD2600 633 VV_point.wdf\n",
      "28 )  UO2 vierge LD2600 633 VV_point_1st_try.wdf\n",
      "29 )  UO2 vierge LD2600 633 VV_point_Copy1.wdf\n",
      "30 )  UO2 vierge LD2600 633 VV_point_Copy2nd.wdf\n",
      "31 )  UO2 vierge LD2600 633 VV_point_Copy3.wdf\n",
      "32 )  UO2 vierge LD2600 633 VV_point_Copy_3rd.wdf\n",
      "33 )  UO2 vierge LD2600 633 VV_point_grande carte.wdf\n",
      "34 )  UO2 vierge LD2600 633 VV_streamline.wdf\n",
      "35 )  UO2 vierge LD2600 633 VV_streamspot.wdf\n",
      "36 )  UO2 vierge LD2600 633 VV_streamspot_Mod.wdf\n",
      "37 )  UO2_LD2-600_Olga_map-Pelletron.wdf\n",
      "38 )  UO2_LD2-600_Olga_map-Pelletron_Copy1.wdf\n",
      "39 )  UO2_LD2-600_Olga_map-Pelletron_Copy2.wdf\n",
      "40 )  YG2UCO15_carto.wdf\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/raoul.missodey/Desktop/Code_python')\n",
    "files = glob('*.wdf')\n",
    "for i, j in enumerate(files):\n",
    "    print(i, ') ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c9e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = files[37] # choose the number of .wdf file you want to use.\n",
    "# load the file\n",
    "da, img = read_WDF(filename, verbose=0) # verbose = 1 or true to have all information about your file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c371ab0",
   "metadata": {},
   "source": [
    "#### Baseline Substraction\n",
    "\n",
    "Two options to substract the baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd38cce",
   "metadata": {},
   "source": [
    "#### 1st option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  substract baseline : you have to modify lambda_value(smoothing parameter), p_value(the penalizing weighting factor)\n",
    "# and lambda1_value(the smoothing parameter for the first derivative of the residual).\n",
    "check_bline = vis.FindBaseline(da,sigma=da.shifts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to see chosen values\n",
    "print(f\"p: {check_bline.p_val:.2e}, lam: {check_bline.lam_val:.2e},lam1: {check_bline.lam1_val:.2e} \")\n",
    "# calculate the basline for all spectrum\n",
    "baseline = cc.baseline_ials(da, p=check_bline.p_val, lam=check_bline.lam_val, lam1= check_bline.lam1_val)\n",
    "# substraction\n",
    "db = da.copy()\n",
    "db.values = db.values - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fbdc0",
   "metadata": {},
   "source": [
    "#### 2nd option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de86239",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_bline = vis.FindBaseline1(da,sigma=da.shifts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d22d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lam: 6.88e+06\n"
     ]
    }
   ],
   "source": [
    "print(f\"lam: {check_bline.lam_val:.2e}\")\n",
    "# calculate the basline for all spectrum\n",
    "baseline = cc.baseline_arpls(da, lam=check_bline.lam_val)\n",
    "# substraction\n",
    "db = da.copy()\n",
    "db.values = db.values - baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5dfebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d13d6552",
   "metadata": {},
   "source": [
    "### AutoEncoder for cosmic ray removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6d56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your firt output dimension must be less than the lenght of the spectra (or the total number of wavenumber)\n",
    "\n",
    "class AutoEncoder(Model):\n",
    "  def __init__(self):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(8, activation=\"relu\")\n",
    "              ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                  tf.keras.layers.Dense(393, activation=\"sigmoid\") # 391 is here the length of each spectra in my data\n",
    "              ])\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa669a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = db.data.copy() # put all intensity (after baseline substraction) in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882759b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X into train and test\n",
    "# test_size = 0.2 means 80% for training and 20% for test\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=228) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c290df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the train and test to use it as validation data when training the neural network\n",
    "# choose normalize method you want to apply : min_max, l1, l2, max, area, robust_scale \n",
    "#dm.values -= np.min(dm.values, axis=-1, keepdims=True)\n",
    "Xn_train = pp.normalize(X_train, method=\"min_max\")\n",
    "Xn_test = pp.normalize(X_test, method=\"min_max\")\n",
    "Xn = pp.normalize(X, method=\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ac86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "99/99 [==============================] - 1s 5ms/step - loss: 0.0922 - val_loss: 0.0566\n",
      "Epoch 2/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.0327\n",
      "Epoch 3/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0306\n",
      "Epoch 4/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0296\n",
      "Epoch 5/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.0292\n",
      "Epoch 6/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0292 - val_loss: 0.0290\n",
      "Epoch 7/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0286 - val_loss: 0.0283\n",
      "Epoch 8/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0286\n",
      "Epoch 9/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0288\n",
      "Epoch 10/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0280\n",
      "Epoch 11/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0283\n",
      "Epoch 12/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0278\n",
      "Epoch 13/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0278\n",
      "Epoch 14/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.0277\n",
      "Epoch 15/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0272\n",
      "Epoch 16/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.0265\n",
      "Epoch 17/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0263\n",
      "Epoch 18/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0264 - val_loss: 0.0262\n",
      "Epoch 19/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.0261\n",
      "Epoch 20/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 21/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 22/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 23/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 24/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0261\n",
      "Epoch 25/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 26/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0260\n",
      "Epoch 27/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.0261\n",
      "Epoch 28/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0259\n",
      "Epoch 29/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0259\n",
      "Epoch 30/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 31/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0259\n",
      "Epoch 32/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0259\n",
      "Epoch 33/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0259\n",
      "Epoch 34/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 35/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0259\n",
      "Epoch 36/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0258\n",
      "Epoch 37/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0259\n",
      "Epoch 38/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 39/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 40/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 41/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 42/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 43/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0257\n",
      "Epoch 44/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0257\n",
      "Epoch 45/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0256\n",
      "Epoch 46/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.0256\n",
      "Epoch 47/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0256\n",
      "Epoch 48/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0256\n",
      "Epoch 49/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 50/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0255\n",
      "Epoch 51/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 52/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 53/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 54/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 55/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 56/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 57/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 58/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 59/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 60/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 61/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 62/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 63/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 64/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 65/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 66/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 67/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 68/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 69/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 70/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 71/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 72/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 73/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0253\n",
      "Epoch 74/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 75/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0253\n",
      "Epoch 76/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 77/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 78/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 79/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 80/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 81/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 82/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 83/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 84/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 85/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 86/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 87/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 88/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0256\n",
      "Epoch 89/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0253\n",
      "Epoch 90/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 91/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 92/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 93/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 94/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0252\n",
      "Epoch 95/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0252\n",
      "Epoch 96/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 97/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0254\n",
      "Epoch 98/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 99/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 100/100\n",
      "99/99 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.0254\n"
     ]
    }
   ],
   "source": [
    "# run this cell to train the model\n",
    "#the first parameter is the training data\n",
    "#the second parameter is the answer (here you can put the same number of train data but without cosmic ray to have a good model)\n",
    "# epochs : nomber of time when the train data passes through the network \n",
    "# batch_size : number of spectra to use at each iteration\n",
    "model = AutoEncoder()\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"min\")\n",
    "model.compile(optimizer='adam', loss=\"mae\")\n",
    "history = model.fit(X_train, X_train, epochs=100, batch_size=64,\n",
    "                    validation_data=(Xn_train, Xn_train),\n",
    "                    shuffle=False\n",
    "                   # callbacks=[early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dc24dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "encoder_out = model.encoder(Xn_test).numpy() #8 unit representation of data\n",
    "decoder_out = model.decoder(encoder_out).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163eea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "_s = np.stack((Xn_test,decoder_out), axis=-1)\n",
    "show_result = vis.ShowSpectra(_s, labels=[\"test_spectra\", \"decoder\"],sigma = da.shifts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c997ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b7d5e670d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot loss curve\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'])\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('mae Loss')\n",
    "ax.legend(['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44d5a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 997us/step\n"
     ]
    }
   ],
   "source": [
    "# apply it for all the data\n",
    "Y = model.predict(Xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3abca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "_s = np.stack((Xn,Y), axis=-1)\n",
    "show_result = vis.ShowSpectra(_s, labels=[\"original spectrum\", \"decoder\"],sigma = da.shifts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c594d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "db1 = db.copy()\n",
    "db1.values = Y\n",
    "db1.values -= np.min(db1.values, axis=-1, keepdims=True)\n",
    "dn = pp.normalize(db1, method=\"min_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef7e49",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1888fa",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f59f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced svd : click inside the score image to have the related spectra, \n",
    "# change the feature_range to normalize the related spectra\n",
    "advanced_svd = vis.SVD(dn, n_components=11, visualize_clean=1, visualize_components=1,\n",
    "                       visualize_err=1,visualize_var=1,feature_range=(-0.075,0.075))\n",
    "dsvd ,var,scores,loadings = advanced_svd.svd_element(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc981987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d313f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec700285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b9aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971b30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232939e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3ef20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbe3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15500f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deaafba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f3156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed027e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2b277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18604d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275f378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20fac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f746a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6765e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bb38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04297544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995b144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3c64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5207d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = model.predict(Xn_test)\n",
    "train_loss = tf.keras.losses.mae(reconstruction,Xn_test)\n",
    "Train_loss = pd.Series(train_loss)\n",
    "fig,ax = plt.subplots()\n",
    "a,b,c = ax.hist(Train_loss,bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe16cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "a,b,c = ax.hist(Train_loss,bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22a8f366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0343474569769878"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = np.mean(train_loss) + 2*np.std(train_loss)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59bb9dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=1555>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tf.math.less(Train_loss , threshold)\n",
    "tf.math.count_nonzero(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
